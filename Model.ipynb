{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40164910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks loaded: 638\n",
      "Pehle chunk ka example:\n",
      "VOLUME | SUPPLEMENT | PAGES S1â€“S322 THE JOURNAL OF CLINICAL AND APPLIED RESEARCH AND EDUCATION JANUA...\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Apni file ka path yahan daalo\n",
    "file_path = r\"diabetes_knowledge_base (1).json\"\n",
    "\n",
    "chunks = []\n",
    "with open(file_path, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "    for item in data:\n",
    "        if 'text' in item and item['text']:\n",
    "            # Har 'text' entry ko ek chunk maan rahe hain\n",
    "            chunks.append(item['text'])\n",
    "\n",
    "print(f\"Total chunks loaded: {len(chunks)}\")\n",
    "print(\"Pehle chunk ka example:\")\n",
    "print(chunks[0][:100] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "909bb1c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings ban gaye!\n",
      "Shape of embeddings: (638, 384)\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Yeh model text ko embeddings mein badlega\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Saare chunks ke embeddings ek saath banao\n",
    "chunk_embeddings = model.encode(chunks)\n",
    "\n",
    "print(\"Embeddings ban gaye!\")\n",
    "print(f\"Shape of embeddings: {chunk_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bcab2d43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector database ban gaya aur embeddings add ho gaye.\n",
      "FAISS index saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# Embeddings ko FAISS ke liye taiyar karo\n",
    "chunk_embeddings_np = np.array(chunk_embeddings).astype('float32')\n",
    "\n",
    "# FAISS index banao aur embeddings usmein add karo\n",
    "index = faiss.IndexFlatL2(chunk_embeddings_np.shape[1])\n",
    "index.add(chunk_embeddings_np)\n",
    "\n",
    "print(\"Vector database ban gaya aur embeddings add ho gaye.\")\n",
    "\n",
    "# Ab isko save kar lo taaki agli baar dobara na banana pade\n",
    "faiss.write_index(index, \"diabetes_faiss_index.bin\")\n",
    "print(\"FAISS index saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6f15d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "embedding_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fae72736",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModelForCausalLM\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgoogle\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcolab\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m userdata \u001b[38;5;66;03m# Corrected: Added this import\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Step 1: GPU availability check\u001b[39;00m\n\u001b[32m      6\u001b[39m device = \u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from google.colab import userdata # Corrected: Added this import\n",
    "\n",
    "# Step 1: GPU availability check\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Step 2: Hugging Face token ko Colab secrets se access karo\n",
    "# Corrected: Fetching the token securely\n",
    "huggingface_token = userdata.get('HUGGING_FACE_TOKEN')\n",
    "\n",
    "# Step 3: Model aur Tokenizer ko load karo\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, token=huggingface_token) # Corrected: Added token\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    low_cpu_mem_usage=True, # Recommended for memory efficiency\n",
    "    load_in_8bit=True, # Recommended for memory efficiency\n",
    "    token=huggingface_token # Corrected: Added token\n",
    ")\n",
    "\n",
    "print(\"Mistral-7B model and tokenizer loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2190e758",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
